# Awesome AI Safety [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

![License](https://img.shields.io/github/license/Giskard-AI/awesome-ai-safety?color=blue)
[![Discord](https://img.shields.io/discord/939190303397666868?label=Discord)](https://gisk.ar/discord)
[![Mastodon](https://img.shields.io/mastodon/follow/109377499153541532?domain=https%3A%2F%2Ffosstodon.org&style=social)](https://fosstodon.org/@Giskard)
[![HitCount](https://hits.dwyl.com/Giskard-AI/awesome-ai-safety.svg?style=flat)](http://hits.dwyl.com/Giskard-AI/awesome-ai-safety)

Figuring out how to make your AI safer? How to avoid ethical biases, errors, privacy leaks or robustness issues in your AI models? 

This repository contains a curated list of papers & technical articles on AI Quality & Safety that should help ðŸ“š

## Table of Contents

You can browse papers by Machine Learning task category, and use hashtags like `#robustness` to explore AI risk types.

1. [Tabular Machine Learning](#tabular-machine-learning)
2. [Natural Language Processing](#natural-language-processing)
3. [Computer Vision](#computer-vision)
4. [Recommendation System](#recommendation-system)
5. [Time Series](#time-series)

## Tabular Machine Learning

TODO

## Natural Language Processing

* [Beyond Accuracy: Behavioral Testing of NLP Models with CheckList](http://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf) (Ribeiro et al., ACL 2020) `#robustness`

## Computer Vision

TODO

## Recommendation System

TODO

## Time Series

TODO
